# gpt_explain_ko
gpt에 대한 논문 해석 

GPT (Generative Pre-trained Transformer) 란 Generative (생성하는) Pre-trained (사전 학습된) Transformer (트랜스포머) 이다.


## GPT

논문 제목:Improving Language Understanding by Generative Pre-Training
- [GPT 논문 보기](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [GPT 코딩 보기](https://github.com/karpathy/minGPT)

## GPT2
논문 제목:Language Models are Unsupervised Multitask Learners
- [GPT2 논문 보기](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT2 코딩 보기](https://github.com/openai/gpt-2)
## GPT3
논문 제목:Language Models are Few-Shot Learners
- [GPT3 논문 보기](https://arxiv.org/pdf/2005.14165.pdf)
- [GPT3 코딩 보기](https://github.com/openai/gpt-3)
## InstructGPT
논문 제목:Training language models to follow instructions with human feedback
- [InstructGPT 논문 보기](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [InstructGPT 코딩 보기]()


[GPT workflow](https://github.com/zhanglina94/gpt_explain_ko/blob/main/img/igpt.png)

## GPT3.5(ChatGPT)

-----
#### Keypoint
###### RLHF:
-  논문 제목:Proximal Policy Optimization Algorithms

###### PPO

-  논문 제목:Proximal Policy Optimization Algorithms
[PPO Algorithm](https://github.com/zhanglina94/gpt_explain_ko/blob/main/img/ppo.png)

###### 
